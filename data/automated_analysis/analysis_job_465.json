{
    "job_id": 465,
    "job_details": {
        "job_id": 465,
        "Organisatienaam": "DeepL",
        "Type adverteerder": "Directe werkgever",
        "Vacaturetitel": "Staff Analytics Engineer",
        "Standplaats": null,
        "Vacaturelink (origineel)": "glassdoor.nl",
        "Contactpersoon": null,
        "Telefoonnummer": null,
        "E-mail": null,
        "Beroep": "Engineer (overig) (m/v/x)",
        "Opleidingsniveau": "Onbekend",
        "Jobfeed feedbacklink": "Meld fout",
        "Functieomschrijving": "As a Staff Analytics Engineer, your main responsibilities and deliverables will be to build and maintain ETL pipelines, create foundational data models in our data warehouse and integrate new data sources using DataOps best practices, including documentation and testing. You will also support other data specialists in their work to make their queries and tables efficient and their use of data effective., We are looking for a data-focused Staff Analytics Engineer to join a team of versatile engineers that build and maintain data models, libraries and the tooling supporting data processing at DeepL. The data platform combines various data sources, both internal and external, and makes them available to our stakeholders across the company: Developers, Product Development, Data Science and Management. You will work in a cross-functional team with product managers, data scientists, data engineers and developers to solve complex technical challenges. You may work with revenue, customer or usage data and gain an analytical understanding of these datasets., * Collaborate with team members to collect business requirements, define successful outcomes, and design data models\n     * Leverage raw data in our data warehouse to build a curated data models\n     * Write production quality ELT code (Python, SQL) with an eye towards performance and maintainability\n     * Create and maintain architecture and systems documentation\n     * Maintain our data catalog as a scalable resource to support self-service and serve as single source of truth for data definition\n     * Provide data modeling expertise to all team members through code reviews, pair programming, and training to help deliver optimal, DRY, and scalable database designs and queries\n     * Become a domain expert in the data you are work with\n     * Improve, manage, and teach code maintainability and performance standards in code submitted and reviewed\n     * Help our team maintain and improve the infrastructure for data collection, processing, management and analysis\n     * Research, design, implement and drive new solutions and software for solving complex data processing problems across all of DeepL\n     * Work on our internal tools to make the data easily accessible to our internal stakeholders, * Diverse and internationally distributed team: joining our team means becoming part of a large, global community with people of more than 90 nationalities. We're more than just colleagues; we're a group of professionals with a shared mission to connect diverse cultures. Our global presence is growing-we've doubled in size nearly every year, with our employees based in the UK, Germany, the Netherlands, Poland, the US, and Japan, and we continue to expand our network.\n     * Open communication, regular feedback: as a language-focused company, we value the importance of clear, honest communication. We value smooth collaboration, direct and actionable feedback, and believe that leading with empathy makes us better together.\n     * Remote work, flexible hours: whether you're near or in our hubs in cities like Cologne, Berlin, Amsterdam, London, Austin, or Tokyo, or prefer the comfort of your own home, you decide where your office is. We offer remote opportunities, flexible working hours and trust in your productivity, all in sync with your team's general locations and time zones to foster effective and seamless collaboration. Our aim is to integrate your work with your lifestyle, ensuring a balance that respects both your needs and our operational requirements.\n     * Regular in-person team events: we bond over vibrant events that are as unique as our team, from local team and business unit gatherings, to new-joiner onboardings, to company-wide events that bring us all together-literally.\n     * Monthly full-day hacking sessions: every month, we have Hack Fridays, where you can spend your time diving into a project you're passionate about and get the opportunity to work with other teams-we value your initiatives, impact, and creativity.\n     * Comprehensive health insurance: your health comes first. With our comprehensive insurance, we'll make sure you're covered from head to toe.\n     * 30 days of annual leave: we value your peace of mind. With 30 days off (excluding public holidays) and access to mental health resources, we make sure you're as strong mentally as you are professionally.\n     * Annual learning budget: because we never stop learning, we've set up an annual budget for your professional development-pick a learning path which contributes to your career development and we'll back you up\n       If this role and our mission resonate with you, but you're hesitant because you don't check all the boxes, don't let that hold you back. At DeepL, it's all about the value you bring and the growth we can foster together. Go ahead, apply-let's discover your potential together. We can't wait to meet you!",
        "Datum gevonden": "2024-05-24",
        "Beroepsgroep": "Engineering (overig)",
        "Beroepsklasse": "Engineering",
        "Contracttype": "Vast contract",
        "Parttime / fulltime": "Fulltime (> 32 uur)",
        "Branche": "Overig / Onbekend",
        "Organisatiegrootte": "Onbekend",
        "Jaar (van datum gevonden)": 2024,
        "Maand (van datum gevonden)": 5,
        "full_text": "Staff Analytics Engineer\n\nAs a Staff Analytics Engineer, your main responsibilities and deliverables will be to build and maintain ETL pipelines, create foundational data models in our data warehouse and integrate new data sources using DataOps best practices, including documentation and testing. You will also support other data specialists in their work to make their queries and tables efficient and their use of data effective., We are looking for a data-focused Staff Analytics Engineer to join a team of versatile engineers that build and maintain data models, libraries and the tooling supporting data processing at DeepL. The data platform combines various data sources, both internal and external, and makes them available to our stakeholders across the company: Developers, Product Development, Data Science and Management. You will work in a cross-functional team with product managers, data scientists, data engineers and developers to solve complex technical challenges. You may work with revenue, customer or usage data and gain an analytical understanding of these datasets., * Collaborate with team members to collect business requirements, define successful outcomes, and design data models\n     * Leverage raw data in our data warehouse to build a curated data models\n     * Write production quality ELT code (Python, SQL) with an eye towards performance and maintainability\n     * Create and maintain architecture and systems documentation\n     * Maintain our data catalog as a scalable resource to support self-service and serve as single source of truth for data definition\n     * Provide data modeling expertise to all team members through code reviews, pair programming, and training to help deliver optimal, DRY, and scalable database designs and queries\n     * Become a domain expert in the data you are work with\n     * Improve, manage, and teach code maintainability and performance standards in code submitted and reviewed\n     * Help our team maintain and improve the infrastructure for data collection, processing, management and analysis\n     * Research, design, implement and drive new solutions and software for solving complex data processing problems across all of DeepL\n     * Work on our internal tools to make the data easily accessible to our internal stakeholders, * Diverse and internationally distributed team: joining our team means becoming part of a large, global community with people of more than 90 nationalities. We're more than just colleagues; we're a group of professionals with a shared mission to connect diverse cultures. Our global presence is growing-we've doubled in size nearly every year, with our employees based in the UK, Germany, the Netherlands, Poland, the US, and Japan, and we continue to expand our network.\n     * Open communication, regular feedback: as a language-focused company, we value the importance of clear, honest communication. We value smooth collaboration, direct and actionable feedback, and believe that leading with empathy makes us better together.\n     * Remote work, flexible hours: whether you're near or in our hubs in cities like Cologne, Berlin, Amsterdam, London, Austin, or Tokyo, or prefer the comfort of your own home, you decide where your office is. We offer remote opportunities, flexible working hours and trust in your productivity, all in sync with your team's general locations and time zones to foster effective and seamless collaboration. Our aim is to integrate your work with your lifestyle, ensuring a balance that respects both your needs and our operational requirements.\n     * Regular in-person team events: we bond over vibrant events that are as unique as our team, from local team and business unit gatherings, to new-joiner onboardings, to company-wide events that bring us all together-literally.\n     * Monthly full-day hacking sessions: every month, we have Hack Fridays, where you can spend your time diving into a project you're passionate about and get the opportunity to work with other teams-we value your initiatives, impact, and creativity.\n     * Comprehensive health insurance: your health comes first. With our comprehensive insurance, we'll make sure you're covered from head to toe.\n     * 30 days of annual leave: we value your peace of mind. With 30 days off (excluding public holidays) and access to mental health resources, we make sure you're as strong mentally as you are professionally.\n     * Annual learning budget: because we never stop learning, we've set up an annual budget for your professional development-pick a learning path which contributes to your career development and we'll back you up\n       If this role and our mission resonate with you, but you're hesitant because you don't check all the boxes, don't let that hold you back. At DeepL, it's all about the value you bring and the growth we can foster together. Go ahead, apply-let's discover your potential together. We can't wait to meet you!"
    },
    "analysis": {
        "profile_classification": {
            "profile": "Ambiguous / Not Relevant",
            "confidence_score": 2,
            "rationale": "The job description focuses heavily on data engineering, data modeling, and ETL pipelines, with a strong emphasis on traditional data warehousing and analytics. While it mentions supporting data scientists, it does not describe core ML/GenAI engineering tasks like model development, fine-tuning, or deployment of generative models."
        },
        "thematic_analysis": {
            "job_tasks": [
                {
                    "task": "Business Understanding",
                    "phrases": [
                        {
                            "text": "collect business requirements",
                            "justification": "This phrase directly indicates the need to understand and gather requirements from the business side to inform technical work."
                        },
                        {
                            "text": "gain an analytical understanding of these datasets",
                            "justification": "This implies understanding the business context and implications of the data being analyzed."
                        }
                    ]
                },
                {
                    "task": "Data Engineering",
                    "phrases": [
                        {
                            "text": "build and maintain ETL pipelines",
                            "justification": "ETL (Extract, Transform, Load) is a core component of data engineering, involving moving and transforming data."
                        },
                        {
                            "text": "integrate new data sources",
                            "justification": "This involves the technical process of connecting and incorporating new data into existing systems."
                        },
                        {
                            "text": "using DataOps best practices, including documentation and testing",
                            "justification": "DataOps is a set of practices for data management and data science that aims to improve the speed and quality of data analytics, closely related to data engineering."
                        },
                        {
                            "text": "build and maintain data models",
                            "justification": "Creating and managing data models is a fundamental aspect of data engineering and data warehousing."
                        },
                        {
                            "text": "tooling supporting data processing",
                            "justification": "This refers to the infrastructure and systems used for processing data, a key area of data engineering."
                        },
                        {
                            "text": "Leverage raw data in our data warehouse to build a curated data models",
                            "justification": "This describes the process of transforming raw data into usable models, a data engineering task."
                        },
                        {
                            "text": "Write production quality ELT code",
                            "justification": "ELT (Extract, Load, Transform) is a variation of ETL, and writing production-quality code for it is a data engineering responsibility."
                        },
                        {
                            "text": "Maintain our data catalog",
                            "justification": "Data cataloging is part of data governance and management, often handled by data engineers."
                        },
                        {
                            "text": "Help our team maintain and improve the infrastructure for data collection, processing, management and analysis",
                            "justification": "This broadly covers the responsibilities of managing the data infrastructure, a core data engineering function."
                        },
                        {
                            "text": "Research, design, implement and drive new solutions and software for solving complex data processing problems",
                            "justification": "This encompasses the design and implementation of systems for data processing, a data engineering task."
                        }
                    ]
                },
                {
                    "task": "Modeling",
                    "phrases": [
                        {
                            "text": "create foundational data models",
                            "justification": "This refers to the design and creation of data structures, which can overlap with data modeling in a broader sense, but here seems focused on warehouse structure rather than predictive models."
                        },
                        {
                            "text": "define successful outcomes, and design data models",
                            "justification": "Designing data models is mentioned, but in the context of business requirements and data warehousing, not necessarily ML models."
                        },
                        {
                            "text": "build a curated data models",
                            "justification": "Similar to 'create foundational data models', this refers to structuring data for analytical purposes."
                        },
                        {
                            "text": "Provide data modeling expertise",
                            "justification": "This highlights expertise in designing data structures and schemas."
                        },
                        {
                            "text": "help deliver optimal, DRY, and scalable database designs and queries",
                            "justification": "Focuses on database design and query optimization, which is related to data modeling but not ML modeling."
                        }
                    ]
                },
                {
                    "task": "Software Development",
                    "phrases": [
                        {
                            "text": "Write production quality ELT code (Python, SQL)",
                            "justification": "Writing code in Python and SQL for production environments is a software development task, albeit focused on data pipelines."
                        },
                        {
                            "text": "Work on our internal tools to make the data easily accessible to our internal stakeholders",
                            "justification": "Developing internal tools involves software development principles to create functional applications."
                        }
                    ]
                },
                {
                    "task": "Operations Engineering (MLOps)",
                    "phrases": [
                        {
                            "text": "using DataOps best practices, including documentation and testing",
                            "justification": "DataOps practices, while related to MLOps, are more focused on the reliability and efficiency of data pipelines and analytics infrastructure."
                        },
                        {
                            "text": "with an eye towards performance and maintainability",
                            "justification": "This relates to the operational quality of the code, ensuring it runs efficiently and is easy to manage, a principle shared with MLOps but also general software engineering."
                        },
                        {
                            "text": "Improve, manage, and teach code maintainability and performance standards in code submitted and reviewed",
                            "justification": "Focuses on code quality and operational aspects, which are relevant to MLOps but also broader software engineering and data engineering."
                        }
                    ]
                }
            ],
            "technologies": [
                {
                    "tech": "Python",
                    "category": "TECH1: Programming Languages"
                },
                {
                    "tech": "SQL",
                    "category": "TECH1: Programming Languages"
                },
                {
                    "tech": "DataOps",
                    "category": "TECH6: MLOps & Data Pipelines"
                }
            ],
            "soft_skills": [
                {
                    "skill": "Collaborate with team members",
                    "category": "SKILL1: Communication & Collaboration"
                },
                {
                    "skill": "collect business requirements",
                    "category": "SKILL1: Communication & Collaboration"
                },
                {
                    "skill": "support other data specialists in their work",
                    "category": "SKILL1: Communication & Collaboration"
                },
                {
                    "skill": "work in a cross-functional team",
                    "category": "SKILL1: Communication & Collaboration"
                },
                {
                    "skill": "pair programming",
                    "category": "SKILL1: Communication & Collaboration"
                },
                {
                    "skill": "training",
                    "category": "SKILL1: Communication & Collaboration"
                },
                {
                    "skill": "teach code maintainability and performance standards",
                    "category": "SKILL1: Communication & Collaboration"
                },
                {
                    "skill": "Open communication, regular feedback",
                    "category": "SKILL1: Communication & Collaboration"
                },
                {
                    "skill": "smooth collaboration",
                    "category": "SKILL1: Communication & Collaboration"
                },
                {
                    "skill": "leading with empathy",
                    "category": "SKILL1: Communication & Collaboration"
                },
                {
                    "skill": "Research, design, implement and drive new solutions",
                    "category": "SKILL3: Problem Solving & Pragmatism"
                },
                {
                    "skill": "solving complex data processing problems",
                    "category": "SKILL3: Problem Solving & Pragmatism"
                },
                {
                    "skill": "Annual learning budget",
                    "category": "SKILL2: Learning & Adaptability"
                },
                {
                    "skill": "don't check all the boxes, don't let that hold you back",
                    "category": "SKILL2: Learning & Adaptability"
                },
                {
                    "skill": "value you bring and the growth we can foster together",
                    "category": "SKILL2: Learning & Adaptability"
                },
                {
                    "skill": "take initiative",
                    "category": "SKILL5: Innovation & Ownership"
                },
                {
                    "skill": "champion new ideas",
                    "category": "SKILL5: Innovation & Ownership"
                }
            ]
        }
    }
}