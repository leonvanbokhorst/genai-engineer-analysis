# AI Terminology Evolution Research - Phase 1 Findings

## Historical Foundations (1950s-2000s)

### The Birth of "Artificial Intelligence" - Dartmouth Conference 1956

**Key Finding**: The term "artificial intelligence" was officially coined at the Dartmouth Summer Research Project on Artificial Intelligence in 1956.

**Primary Source**: Dartmouth College official documentation

**Key Participants**:
- John McCarthy (Dartmouth College) - Primary organizer and term coiner
- Marvin Minsky (Harvard University)
- Nathaniel Rochester (IBM Corporation)
- Claude Shannon (Bell Telephone Laboratories)

**Original Definition/Conjecture**: According to John McCarthy's proposal, the conference was "to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it."

**Context and Goals**: 
- The researchers aimed to make machines more cognizant
- They wanted to lay out a framework to better understand human intelligence
- The conference was seminal in establishing AI as a field of research

**Historical Significance**: 
- This was the founding event of artificial intelligence as an academic discipline
- The conference not only coined the term but coalesced an entire field of study
- Over 100 researchers later gathered for AI@50 in 2006 to commemorate the anniversary

**URL**: https://home.dartmouth.edu/about/artificial-intelligence-ai-coined-dartmouth



### Detailed Context from Dartmouth Workshop (Wikipedia)

**Background and Naming Decision**:
- In the early 1950s, there were various competing names for the field: "cybernetics," "automata theory," and "complex information processing"
- John McCarthy deliberately chose "Artificial Intelligence" for its neutrality
- He wanted to avoid narrow focus on automata theory and cybernetics (which was heavily focused on analog feedback)
- He also wanted to avoid having to accept Norbert Wiener as a guru or argue with him

**The Formal Proposal (September 2, 1955)**:
The proposal that introduced the term stated:
> "We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves."

**Key Areas Identified** (still relevant today):
- Computers
- Natural language processing  
- Neural networks
- Theory of computation
- Abstraction
- Creativity

**Workshop Structure and Outcomes**:
- Lasted 6-8 weeks (June 18 - August 17, 1956)
- Called "the Constitutional Convention of AI"
- Was essentially an extended brainstorming session
- Not a directed research project but covered many topics
- Initiated several key directions:
  - Rise of symbolic methods
  - Systems focused on limited domains (early expert systems)
  - Deductive systems versus inductive systems

**Notable Participants** (20 total):
- Ray Solomonoff, Marvin Minsky, John McCarthy (stayed full-time)
- Claude Shannon, Nathaniel Rochester, Oliver Selfridge
- Julian Bigelow, W. Ross Ashby, W.S. McCulloch
- John Nash, Herbert Simon, Allen Newell, Arthur Samuel

**Contemporary Assessment**: Arthur Samuel described it as "very interesting, very stimulating, very exciting"


### AI Winters and Terminology Evolution (1970s-1990s)

**AI Winter Periods and Their Impact on Terminology**:

**First AI Winter (1974-1980)**:
- Term "AI winter" first appeared in 1984 at AAAI meeting
- Roger Schank and Marvin Minsky warned of a "nuclear winter" effect
- Chain reaction: pessimism in AI community → press → funding cuts → end of research
- Major setbacks included:
  - 1966: Failure of machine translation projects
  - 1969: Criticism of perceptrons (single-layer neural networks)
  - 1973: Lighthill report caused large decrease in UK AI research
  - 1973-74: DARPA cutbacks to academic AI research

**Second AI Winter (1987-2000)**:
- 1987: Collapse of LISP machine market
- 1988: Strategic Computing Initiative cutbacks
- 1990s: Many expert systems abandoned
- 1990s: End of Fifth Generation computer project

**Expert Systems Era (1980s) - Knowledge-Based AI**:

**Key Terminology Shift**: From general problem-solving to domain-specific knowledge systems

**Formal Introduction (1965)**:
- Stanford Heuristic Programming Project led by Edward Feigenbaum ("father of expert systems")
- Key early systems: MYCIN (medical diagnosis), DENDRAL (molecular identification)

**Core Philosophy**: "Intelligent systems derive their power from the knowledge they possess rather than from the specific formalisms and inference schemes they use" - Edward Feigenbaum

**Architectural Terminology**:
- Knowledge base (facts and rules)
- Inference engine (reasoning mechanism)
- If-then rules
- Production rule systems

**Significance**: Expert systems became "some of the first truly successful forms of artificial intelligence (AI) software"

**Programming Languages**: LISP and Prolog became associated with AI development

**European Focus**: Production rule systems and rule-based programming using formal logic

**Impact on AI Definition**: Shifted focus from general-purpose problem solvers to domain-specific expertise capture and reasoning systems


### Symbolic AI vs Connectionist AI - The Great Divide

**GOFAI (Good Old-Fashioned Artificial Intelligence)**:

**Term Origin**: Coined by philosopher John Haugeland in his 1985 book "Artificial Intelligence: The Very Idea"

**Definition**: Classical symbolic AI, as opposed to neural networks, situated robotics, or neuro-symbolic AI

**Core Philosophy**: Based on Western rationalist tradition that abstract reason is the "highest" faculty and most essential part of intelligence

**Key Assumptions** (Haugeland's criteria):
1. "Our ability to deal with things intelligently is due to our capacity to think about them reasonably (including sub-conscious thinking)"
2. "Our capacity to think about things reasonably amounts to a faculty for internal 'automatic' symbol manipulation"

**Physical Symbol Systems Hypothesis** (Simon & Newell, 1963):
"A physical symbol system has the necessary and sufficient means for general intelligent action"

**Symbolic AI Characteristics**:
- Uses discrete physical symbols with definite semantics (like <cat> and <mat>)
- Governed by formal rules for symbol manipulation
- Successful in high-level reasoning: logical deduction, algebra, geometry, spatial reasoning
- Expressed reasoning in precise English sentences like humans
- Does NOT include cybernetics, perceptrons, dynamic programming, or neural networks

**Connectionist AI Characteristics**:
- Based on neural networks and distributed processing
- Learning through experience and pattern recognition
- Uses connections and weights rather than explicit symbols
- Emerged strongly in 2012 after earlier development in 1940s-1960s

**Historical Impact**:
- GOFAI inspired the cognitive revolution of the 1960s
- Led to founding of cognitive science as academic field
- Influenced philosophical theories of computationalism and functionalism
- Created the foundation for expert systems in the 1980s

**Philosophical Divide**:
- Rationalist tradition (Plato, Aristotle, Enlightenment) supported symbolic approach
- Continental philosophy (Nietzsche, Heidegger) criticized rationalism, supporting more intuitive approaches
- Critics like Hubert Dreyfus argued high-level reasoning was limited and error-prone

**Modern Synthesis**:
- Current AI researchers believe deep learning and neuro-symbolic AI synthesis will be required for general intelligence
- Russell & Norvig noted GOFAI's qualification problem - difficulty capturing every contingency in logical rules
- Post-1980s symbolic AI incorporated probabilistic reasoning, non-monotonic reasoning, and machine learning

